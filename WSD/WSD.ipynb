{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import semcor\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk import Tree\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[[c for c in s] for s in semcor.tagged_sents(tag='both')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Preprocessing\n",
    "The following code contains a series of functions to extract and preprocess the sentences contained in SemCor.\\\n",
    "This way, it will be easier to access the sentences annotated with POS tags and synsets and extract a random word to disambiguate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert nltk pos tag to wordnet pos tag\n",
    "def convert_pos(pos):\n",
    "    pos_map = {'J': 'a', 'N': 'n', 'R': 'r', 'V': 'v', 'M': 'v'}\n",
    "    return pos_map.get(pos[0], pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converter from SemCor to dictionary\n",
    "# given an annotated sentence from the SemCor corpus in the form of a list of trees\n",
    "# returns a list of dictionaries with the word, the POS tag, and the synset\n",
    "# all stopwords and punctuation are ignored\n",
    "# the initial trees for words with lemma are in the form ROOT[label(lemma)]-->child[label(posTag)]-->child[word]\n",
    "# the initial trees for words without lemma are in the form ROOT[label(posTag)]-->child[word]\n",
    "def extract_info(list):\n",
    "    token_list = []\n",
    "    for tree in list:\n",
    "        token={\"word\":None,\"pos\":None,\"syn\":None}\n",
    "        label=tree.label()\n",
    "        if isinstance(label,Lemma): # if the token has a lemma, I derive the synset\n",
    "            token[\"syn\"]=label.synset()\n",
    "            tree=tree[0] # explore the child that contains the word and the POS tag\n",
    "\n",
    "        # ignore punctuation and compound words/entities (e.g., New York)\n",
    "        if not isinstance(tree[0],Tree) and label is not None:\n",
    "            token[\"pos\"]=convert_pos(tree.label())\n",
    "            token[\"word\"]=tree[0].lower()\n",
    "            token_list.append(token)\n",
    "\n",
    "    return token_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a random word and the sentence from which it was extracted; if only_nouns=True, it necessarily returns a noun\n",
    "def get_random_word(data,only_nouns=False):\n",
    "    found=False\n",
    "    while not found:\n",
    "        sentence=random.choice(data)\n",
    "        token_list=extract_info(sentence)\n",
    "        content_word=[token for token in token_list if token[\"syn\"]is not None]#exclude stopwords\n",
    "        for token in content_word:\n",
    "            if wn.synsets(token[\"word\"]) != []:#if the word has at least one synset\n",
    "                if not only_nouns or token[\"pos\"]==\"n\":\n",
    "                        return (token,token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Implementazione WSD\n",
    "The following code constitutes the main part of the project, containing the functions to obtain the context, the signature, to implement the overlap, and the Lesk algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocesses the data by removing punctuation, tokenizing, and lemmatizing\n",
    "def preprocess(sentence): \n",
    "    if isinstance(sentence,str): # if it is a string, tokenize it into a list\n",
    "        sentence=word_tokenize(sentence)\n",
    "    sentence=[w.lower() for w in sentence if w not in string.punctuation]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens=[lemmatizer.lemmatize(w) for w in sentence]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a list of synsets and the annotated sentences from SemCor\n",
    "# checks each sentence that contains a certain synset\n",
    "# returns a dictionary, where each synset is associated with a list of tokenized sentences in which it appears\n",
    "# corpus_examples= {syn1: [[example1],[example2],[example3]], syn2:[...]}\n",
    "def get_corpus_examples(synsets,data):\n",
    "    corpus_examples={syn.name():[] for syn in synsets}\n",
    "    for sentence in data:\n",
    "        token_list=extract_info(sentence)\n",
    "        for token in token_list:\n",
    "            syn=token[\"syn\"]\n",
    "            if syn is not None and syn in synsets:\n",
    "                corpus_examples[syn.name()].append([t[\"word\"]for t in token_list])\n",
    "                break\n",
    "    return corpus_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each word in word_list increments the number of documents containing the word\n",
    "# if the word is not present, it is added\n",
    "def add_to_signature(signature_dict,word_list):\n",
    "    for word in set(word_list): # duplicates in the same sentence are not considered (?)\n",
    "        signature_dict[word]=signature_dict.get(word,0)+1\n",
    "    return signature_dict\n",
    "\n",
    "# returns the signature of a synset\n",
    "# the signature is a dictionary that associates each word with its weight (idf) calculated as idf_i=log(Ndoc/Nd_i)\n",
    "def get_signature(synset,corpus_examples=None,gemini_examples=None):\n",
    "    signature={}\n",
    "\n",
    "    #----------------the definition and examples from wordnet are added---------------\n",
    "    Ndoc=1 # synset definition\n",
    "    signature=add_to_signature(signature,preprocess(synset.definition())) # description of the synset\n",
    "    for wn_ex in synset.examples(): # examples from wordnet\n",
    "        signature=add_to_signature(signature,preprocess(wn_ex))\n",
    "        Ndoc+=1\n",
    "\n",
    "    #----------------the examples from the corpus are added-----------------------\n",
    "    if corpus_examples: # if there are corpus examples present\n",
    "        for corpus_ex in corpus_examples: # sentences in SemCor containing the synset\n",
    "            signature=add_to_signature(signature,preprocess(corpus_ex))\n",
    "            Ndoc+=1\n",
    "\n",
    "    #----------------the examples from Gemini are added-----------------------\n",
    "    if gemini_examples: # if there are examples from Gemini present\n",
    "        for gemini_ex in gemini_examples:\n",
    "            signature=add_to_signature(signature,preprocess(gemini_ex))\n",
    "            Ndoc+=1\n",
    "            \n",
    "    # calculation of idf\n",
    "    for word in signature:\n",
    "        signature[word]=np.log(Ndoc/signature[word])\n",
    "    sorted_signature = dict(sorted(signature.items(), key=lambda item: item[1],reverse=True))\n",
    "    return sorted_signature\n",
    "\n",
    "# returns the context of a word\n",
    "# the context consists of the list of lemmatized words of the sentence in which the word to be disambiguated is located\n",
    "# if the word is a content word, I use its pos tag to assist the lemmatizer\n",
    "# otherwise it is a stopword and is not lemmatized\n",
    "def get_context(token_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(t[\"word\"], t[\"pos\"]) if t[\"syn\"] is not None else t[\"word\"] for t in token_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the overlap between the context and the signature of a synset\n",
    "# if weighted=True calculates the sum of the weights of the common words\n",
    "# if weighted=False calculates the number of common words excluding stopwords\n",
    "def compute_overlap(signature,context,weighted=False):\n",
    "    if weighted:\n",
    "        return sum([signature.get(word,0) for word in context])\n",
    "    else:\n",
    "        stopword = stopwords.words('english')\n",
    "        #remove stopwords\n",
    "        context= [w for w in context if w.casefold() not in stopword]\n",
    "        signature= {k:v for k,v in signature.items() if k not in stopword}\n",
    "        return len(set(signature.keys()).intersection(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of the Lesk algorithm\n",
    "# returns the synset with the maximum overlap between the signature and the context\n",
    "def lesk(word, pos, sentence, data=None, gemini=False):\n",
    "    synsets = wn.synsets(word, pos)\n",
    "    if synsets == []: synsets = wn.synsets(word)  # if the synset with the specified pos tag is not found, use all synsets\n",
    "    max_overlap = 0\n",
    "    best_syns = synsets[0]\n",
    "    context = get_context(sentence)\n",
    "\n",
    "    # -----------------if specified, extract additional examples (corpus and/or gemini)---------------------\n",
    "    if data: \n",
    "        corpus_examples = get_corpus_examples(synsets, data)\n",
    "    else: \n",
    "        corpus_examples = None\n",
    "    if gemini:\n",
    "        gemini_examples = get_gemini_examples(synsets)  # optional part at the end\n",
    "    else: \n",
    "        gemini_examples = None\n",
    "\n",
    "    # ----------------search for the synset with the greatest overlap---------------------\n",
    "    for syn in synsets:\n",
    "        signature = get_signature(syn, \n",
    "                                  corpus_examples.get(syn.name(), None) if corpus_examples else None,\n",
    "                                  gemini_examples.get(syn.name(), None) if gemini_examples else None)\n",
    "        overlap = compute_overlap(signature, context, weighted=bool(data))\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_syns = syn\n",
    "\n",
    "    return best_syns\n",
    "\n",
    "# baseline, returns the most frequent synset\n",
    "def naive_wsd(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    if synsets == []: \n",
    "        return None\n",
    "    else: \n",
    "        return synsets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# word:  son\n",
      "# sentence:  ['they', 'have', 'a', 'son', 'and', 'a', 'daughter', 'of']\n",
      "# extracted context:  ['they', 'have', 'a', 'son', 'and', 'a', 'daughter', 'of']\n",
      "# signature of each synset (No corpus examples):\n",
      " - son.n.01  overlap:  1  signature:  {'offspring': 1.0986122886681098, 'human': 1.0986122886681098, 'male': 1.0986122886681098, 'their': 1.0986122886681098, 'son': 1.0986122886681098, 'famous': 1.0986122886681098, 'judge': 1.0986122886681098, 'became': 1.0986122886681098, 'boy': 1.0986122886681098, 'than': 1.0986122886681098, 'his': 1.0986122886681098, 'taller': 1.0986122886681098, 'is': 1.0986122886681098, 'he': 1.0986122886681098, 'a': 0.4054651081081644}\n",
      " - son.n.02  overlap:  0  signature:  {'jesus': 0.0, 'of': 0.0, 'divine': 0.0, 'second': 0.0, 'trinity': 0.0, 'the': 0.0, 'in': 0.0, 'word': 0.0, 'person': 0.0, 'god': 0.0, 'incarnate': 0.0}\n",
      "\n",
      "# signature of each synset (with corpus examples):\n",
      " - son.n.01  overlap: 11.207  signature:  {'offspring': 4.1588830833596715, 'human': 4.1588830833596715, 'male': 4.1588830833596715, 'judge': 4.1588830833596715, 'became': 4.1588830833596715, 'taller': 4.1588830833596715, 'miserable': 4.1588830833596715, 'particular': 4.1588830833596715, 'word': 4.1588830833596715, 'everybody': 4.1588830833596715, 'should': 4.1588830833596715, 'youngster': 4.1588830833596715, 'make': 4.1588830833596715, 'broadway': 4.1588830833596715, 'nearly': 4.1588830833596715, 'looked': 4.1588830833596715, 'worried': 4.1588830833596715, 'still': 4.1588830833596715, 'anticipate': 4.1588830833596715, 'easy': 4.1588830833596715, 'sprawling': 4.1588830833596715, 'jumping': 4.1588830833596715, 'alert': 4.1588830833596715, 'familiarity': 4.1588830833596715, 'completely': 4.1588830833596715, 'eye': 4.1588830833596715, 'bed': 4.1588830833596715, 'stand': 4.1588830833596715, 'self-consciously': 4.1588830833596715, 'foot': 4.1588830833596715, 'upstairs': 4.1588830833596715, 'critical': 4.1588830833596715, 'blame': 4.1588830833596715, 'everything': 4.1588830833596715, 'material': 4.1588830833596715, 'examined': 4.1588830833596715, 'care': 4.1588830833596715, 'something': 4.1588830833596715, 'seeking': 4.1588830833596715, 'illness': 4.1588830833596715, 'gather': 4.1588830833596715, 'tell': 4.1588830833596715, 'ye': 4.1588830833596715, 'may': 4.1588830833596715, 'rosebud': 4.1588830833596715, 'afford': 4.1588830833596715, 'clad': 4.1588830833596715, 'black': 4.1588830833596715, 'arm': 4.1588830833596715, 'relax': 4.1588830833596715, 'embrace': 4.1588830833596715, 'stern': 4.1588830833596715, 'patriot': 4.1588830833596715, 'disagreed': 4.1588830833596715, 'universe': 4.1588830833596715, 'create': 4.1588830833596715, 'alone': 4.1588830833596715, 'intensely': 4.1588830833596715, 'alive': 4.1588830833596715, 'anguished': 4.1588830833596715, 'felt': 4.1588830833596715, 'project': 4.1588830833596715, 'possible': 4.1588830833596715, 'therefore': 4.1588830833596715, 'sculpture': 4.1588830833596715, 'anything': 4.1588830833596715, 'friend': 4.1588830833596715, 'showed': 4.1588830833596715, 'sitting': 4.1588830833596715, 'drawing': 4.1588830833596715, 'grasped': 4.1588830833596715, 'roughed': 4.1588830833596715, 'holding': 4.1588830833596715, 'promised': 4.1588830833596715, 'end': 4.1588830833596715, 'quick': 4.1588830833596715, 'model': 4.1588830833596715, 'well': 4.1588830833596715, 'together': 4.1588830833596715, 'lean': 4.1588830833596715, 'sensitive': 4.1588830833596715, 'week': 4.1588830833596715, 'from': 4.1588830833596715, 'yet': 4.1588830833596715, 'figure': 4.1588830833596715, 'experience': 4.1588830833596715, 'putting': 4.1588830833596715, 'spent': 4.1588830833596715, 'remembered': 4.1588830833596715, 'even': 4.1588830833596715, 'lap': 4.1588830833596715, 'concentrated': 4.1588830833596715, 'duty': 4.1588830833596715, 'left': 4.1588830833596715, 'decision': 4.1588830833596715, 'organizing': 4.1588830833596715, 'dear': 4.1588830833596715, 'grief': 4.1588830833596715, 'sorrow': 4.1588830833596715, 'militia': 4.1588830833596715, 'poor': 4.1588830833596715, 'wrote': 4.1588830833596715, 'morning': 4.1588830833596715, 'smiling': 4.1588830833596715, 'foolish': 4.1588830833596715, 'large': 4.1588830833596715, 'stood': 4.1588830833596715, 'big': 4.1588830833596715, 'goddamned': 4.1588830833596715, 'poland': 4.1588830833596715, 'hate': 4.1588830833596715, 'employment': 4.1588830833596715, 'supplementing': 4.1588830833596715, 'sea': 4.1588830833596715, 'livelihood': 4.1588830833596715, 'elderly': 4.1588830833596715, 'small': 4.1588830833596715, 'fishing': 4.1588830833596715, 'lived': 4.1588830833596715, 'fourth': 4.1588830833596715, 'rented': 4.1588830833596715, 'farmer': 4.1588830833596715, 'seasonal': 4.1588830833596715, 'divided': 4.1588830833596715, 'coast': 4.1588830833596715, 'wished': 4.1588830833596715, 'winter': 4.1588830833596715, 'came': 4.1588830833596715, 'thought': 4.1588830833596715, 'awaken': 4.1588830833596715, 'bringing': 4.1588830833596715, 'every': 4.1588830833596715, 'way': 4.1588830833596715, 'review': 4.1588830833596715, 'coming': 4.1588830833596715, 'yes': 4.1588830833596715, 'guest': 4.1588830833596715, 'hers': 4.1588830833596715, 'minute': 4.1588830833596715, 'burning': 4.1588830833596715, 'moment': 4.1588830833596715, 'dinner': 4.1588830833596715, 'few': 4.1588830833596715, 'stove': 4.1588830833596715, 'bride': 4.1588830833596715, 'house': 4.1588830833596715, 'no': 4.1588830833596715, 'till': 4.1588830833596715, 'room': 4.1588830833596715, 'tiny': 4.1588830833596715, 'how': 4.1588830833596715, 'granny': 4.1588830833596715, 'sit': 4.1588830833596715, 'porch': 4.1588830833596715, 'our': 4.1588830833596715, 'summer': 4.1588830833596715, 'roast': 4.1588830833596715, 'upon': 4.1588830833596715, 'rich': 4.1588830833596715, 'concrete': 4.1588830833596715, 'himself': 4.1588830833596715, 'coolness': 4.1588830833596715, 'day': 4.1588830833596715, 'dreaming': 4.1588830833596715, 'down': 4.1588830833596715, 'taking': 4.1588830833596715, 'step': 4.1588830833596715, 'threw': 4.1588830833596715, 'belly': 4.1588830833596715, 'couple': 4.1588830833596715, 'social': 4.1588830833596715, 'accept': 4.1588830833596715, 'roving': 4.1588830833596715, 'order': 4.1588830833596715, 'happy': 4.1588830833596715, 'truculent': 4.1588830833596715, 'fighting': 4.1588830833596715, 'new': 4.1588830833596715, 'ideal': 4.1588830833596715, 'existence': 4.1588830833596715, 'discipline': 4.1588830833596715, 'here': 4.1588830833596715, 'economic': 4.1588830833596715, 'world': 4.1588830833596715, 'very': 4.1588830833596715, 'proved': 4.1588830833596715, 'knowledge': 4.1588830833596715, 'amateur': 4.1588830833596715, 'imparted': 4.1588830833596715, 'engraver': 4.1588830833596715, 'painter': 4.1588830833596715, 'professional': 4.1588830833596715, 'love': 4.1588830833596715, 'hunting': 4.1588830833596715, 'landscape': 4.1588830833596715, 'numerous': 4.1588830833596715, 'sure': 4.1588830833596715, 'londoner': 4.1588830833596715, 'foraging': 4.1588830833596715, 'cleaning': 4.1588830833596715, 'menial': 4.1588830833596715, 'retain': 4.1588830833596715, 'chore': 4.1588830833596715, 'early': 4.1588830833596715, 'shining': 4.1588830833596715, 'war': 4.1588830833596715, 'shoe': 4.1588830833596715, 'camp': 4.1588830833596715, 'clothes': 4.1588830833596715, 'laundering': 4.1588830833596715, 'perform': 4.1588830833596715, 'body': 4.1588830833596715, 'planter': 4.1588830833596715, 'negro': 4.1588830833596715, 'uncommon': 4.1588830833596715, 'sicilian': 4.1588830833596715, 'plasterer': 4.1588830833596715, 'close': 4.1588830833596715, 'immigrant': 4.1588830833596715, 'born': 4.1588830833596715, 'poverty': 4.1588830833596715, 'irish': 4.1588830833596715, 'warrant': 4.1588830833596715, 'lower-middle': 4.1588830833596715, 'resource': 4.1588830833596715, 'out-of-town': 4.1588830833596715, 'financial': 4.1588830833596715, 'typically': 4.1588830833596715, 'attend': 4.1588830833596715, 'priority': 4.1588830833596715, 'education': 4.1588830833596715, 'more': 4.1588830833596715, 'attitude': 4.1588830833596715, 'protective': 4.1588830833596715, 'toward': 4.1588830833596715, 'already': 4.1588830833596715, 'academy': 4.1588830833596715, 'can': 4.1588830833596715, 'select': 4.1588830833596715, 'strict': 4.1588830833596715, 'impunity': 4.1588830833596715, 'please': 4.1588830833596715, 'liberal': 4.1588830833596715, 'prominent': 4.1588830833596715, 'tell-tale': 4.1588830833596715, 'heir': 4.1588830833596715, 'finger': 4.1588830833596715, 'marries': 4.1588830833596715, 'shadow': 4.1588830833596715, 'half-moon': 4.1588830833596715, 'psychology': 4.1588830833596715, 'jolly': 4.1588830833596715, 'round-faced': 4.1588830833596715, 'distinguished': 4.1588830833596715, 'mustache': 4.1588830833596715, 'rabbi': 4.1588830833596715, 'whose': 4.1588830833596715, 'punster': 4.1588830833596715, 'friendly': 4.1588830833596715, 'fellow': 4.1588830833596715, 'also': 4.1588830833596715, 'tease': 4.1588830833596715, 'field': 4.1588830833596715, 'baltimore': 4.1588830833596715, 'special': 4.1588830833596715, 'pupil': 4.1588830833596715, 'most': 4.1588830833596715, 'home': 4.1588830833596715, 'near': 4.1588830833596715, 'chopin': 4.1588830833596715, 'studied': 4.1588830833596715, 'delight': 4.1588830833596715, 'visit': 4.1588830833596715, 'movingly': 4.1588830833596715, 'philadelphia': 4.1588830833596715, 'recited': 4.1588830833596715, 'played': 4.1588830833596715, 'visiting': 4.1588830833596715, 'soiree': 4.1588830833596715, 'do': 4.1588830833596715, 'bent': 4.1588830833596715, 'study': 4.1588830833596715, 'trumpet': 4.1588830833596715, 'started': 4.1588830833596715, 'musical': 4.1588830833596715, 'evidencing': 4.1588830833596715, 'following': 4.1588830833596715, 'music': 4.1588830833596715, 'waited': 4.1588830833596715, '720': 4.1588830833596715, 'earned': 4.1588830833596715, '1961': 4.1588830833596715, '16': 4.1588830833596715, 'watched': 4.1588830833596715, 'almost': 4.1588830833596715, 'hurried': 4.1588830833596715, 'out': 4.1588830833596715, 'hall': 4.1588830833596715, 'killed': 4.1588830833596715, 'finished': 4.1588830833596715, 'into': 4.1588830833596715, 'door': 4.1588830833596715, 'slammed': 4.1588830833596715, 'particularly': 4.1588830833596715, 'apparently': 4.1588830833596715, 'forced': 4.1588830833596715, 'busboy': 4.1588830833596715, 'give': 4.1588830833596715, 'right': 4.1588830833596715, 'month': 4.1588830833596715, 'pickup': 4.1588830833596715, 'send': 4.1588830833596715, 'join': 4.1588830833596715, 'wife': 4.1588830833596715, 'outfit': 4.1588830833596715, 'bos': 4.1588830833596715, 'failure': 4.1588830833596715, 'too': 4.1588830833596715, 'under': 4.1588830833596715, 'influence': 4.1588830833596715, 'five': 4.1588830833596715, 'forgotten': 4.1588830833596715, 'hell': 4.1588830833596715, \"'m\": 4.1588830833596715, 'going': 4.1588830833596715, 'others': 4.1588830833596715, 'failed': 4.1588830833596715, 'withdraw': 4.1588830833596715, 'trouble': 4.1588830833596715, 'bluff': 4.1588830833596715, 'ran': 4.1588830833596715, 'another': 4.1588830833596715, 'bawhs': 4.1588830833596715, 'began': 4.1588830833596715, 'smilingly': 4.1588830833596715, 'about': 4.1588830833596715, 'younguh': 4.1588830833596715, 'declined': 4.1588830833596715, 'suddenly': 4.1588830833596715, 'age': 4.1588830833596715, 'never': 4.1588830833596715, 'believed': 4.1588830833596715, 'drink': 4.1588830833596715, 'aw': 4.1588830833596715, 'f': 4.1588830833596715, 'fathuh': 4.1588830833596715, 'aftuh': 4.1588830833596715, 'bawh': 4.1588830833596715, 'ah': 4.1588830833596715, 'thet': 4.1588830833596715, 'majuh': 4.1588830833596715, 'same': 4.1588830833596715, 'just': 4.1588830833596715, 'mah': 4.1588830833596715, 'tole': 4.1588830833596715, 'doa': 4.1588830833596715, 'gay-ess': 4.1588830833596715, 'ovuh': 4.1588830833596715, 'wohd': 4.1588830833596715, 'yeah': 4.1588830833596715, 'n': 4.1588830833596715, 'hevin': 4.1588830833596715, 'uttuh': 4.1588830833596715, 'nahce': 4.1588830833596715, 'think': 4.1588830833596715, 'behahn': 4.1588830833596715, 'coudn': 4.1588830833596715, 't': 4.1588830833596715, 'did': 4.1588830833596715, 'occasional': 4.1588830833596715, 'd': 4.1588830833596715, 'laughed': 4.1588830833596715, 'back': 4.1588830833596715, 'their': 3.4657359027997265, 'famous': 3.4657359027997265, 'life': 3.4657359027997265, 'all': 3.4657359027997265, 'desire': 3.4657359027997265, 'always': 3.4657359027997265, 'look': 3.4657359027997265, 'my': 3.4657359027997265, 'now': 3.4657359027997265, 'dead': 3.4657359027997265, 'jesus': 3.4657359027997265, 'not': 3.4657359027997265, 'when': 3.4657359027997265, 'young': 3.4657359027997265, 'two': 3.4657359027997265, 'though': 3.4657359027997265, 'strong': 3.4657359027997265, 'death': 3.4657359027997265, 'nothing': 3.4657359027997265, 'land': 3.4657359027997265, 'south': 3.4657359027997265, 'good': 3.4657359027997265, 'company': 3.4657359027997265, 'say': 3.4657359027997265, 'thing': 3.4657359027997265, 'there': 3.4657359027997265, 'them': 3.4657359027997265, 'farm': 3.4657359027997265, 'between': 3.4657359027997265, 'were': 3.4657359027997265, 'school': 3.4657359027997265, 'three': 3.4657359027997265, 'own': 3.4657359027997265, 'get': 3.4657359027997265, 'where': 3.4657359027997265, 'want': 3.4657359027997265, 'second': 3.4657359027997265, 'one': 3.4657359027997265, 'quarter': 3.4657359027997265, 'jewish': 3.4657359027997265, 'sent': 3.4657359027997265, 'ha': 3.4657359027997265, 'far': 3.4657359027997265, \"'ve\": 3.4657359027997265, 'your': 3.4657359027997265, 'little': 3.4657359027997265, 'piano': 3.4657359027997265, 'ten': 3.4657359027997265, 'until': 3.4657359027997265, 'so': 3.4657359027997265, 'year': 3.4657359027997265, 'got': 3.4657359027997265, 'time': 3.4657359027997265, 'or': 3.4657359027997265, 'told': 3.4657359027997265, 'been': 3.4657359027997265, 'me': 3.4657359027997265, 'onleh': 3.4657359027997265, 'they': 3.060270794691562, 'said': 3.060270794691562, 'come': 3.060270794691562, 'while': 3.060270794691562, 'then': 3.060270794691562, 'what': 3.060270794691562, 'only': 3.060270794691562, 'enough': 3.060270794691562, 'this': 3.060270794691562, 'old': 3.060270794691562, 'i': 3.060270794691562, 'college': 3.060270794691562, 'took': 3.060270794691562, 'married': 3.060270794691562, 'that': 3.060270794691562, 'by': 3.060270794691562, 'will': 3.060270794691562, 'family': 3.060270794691562, 'after': 3.060270794691562, 'than': 2.772588722239781, 'is': 2.772588722239781, 'face': 2.772588722239781, 'father': 2.772588722239781, 'if': 2.772588722239781, 'first': 2.772588722239781, 'go': 2.772588722239781, 'baby': 2.772588722239781, 'have': 2.5494451709255714, 'could': 2.5494451709255714, 'an': 2.5494451709255714, 'girl': 2.5494451709255714, 'you': 2.5494451709255714, 'daughter': 2.367123614131617, 'who': 2.367123614131617, 'but': 2.367123614131617, 'had': 2.2129729343043585, 'mother': 2.2129729343043585, 'be': 2.0794415416798357, 'at': 2.0794415416798357, 'him': 2.0794415416798357, 'on': 1.9616585060234524, 'her': 1.9616585060234524, 'for': 1.8562979903656263, 'with': 1.8562979903656263, 'it': 1.8562979903656263, 'would': 1.7609878105613013, 'she': 1.7609878105613013, 'he': 1.519825753744413, \"'s\": 1.519825753744413, 'boy': 1.4508328822574619, 'wa': 1.3256697393034558, 'in': 1.2144441041932312, 'to': 1.1143606456362487, 'of': 0.9808292530117262, 'his': 0.9400072584914712, 'a': 0.6931471805599453, 'and': 0.5753641449035618, 'the': 0.47000362924573563, 'son': 0.28768207245178085}\n",
      " - son.n.02  overlap: 1.5041  signature:  {'jesus': 1.0986122886681098, 'divine': 1.0986122886681098, 'second': 1.0986122886681098, 'trinity': 1.0986122886681098, 'word': 1.0986122886681098, 'person': 1.0986122886681098, 'incarnate': 1.0986122886681098, 'be': 1.0986122886681098, 'u': 1.0986122886681098, 'brother': 1.0986122886681098, 'united': 1.0986122886681098, 'our': 1.0986122886681098, 'intercede': 1.0986122886681098, 'fold': 1.0986122886681098, 'thy': 1.0986122886681098, 'may': 1.0986122886681098, 'with': 1.0986122886681098, 'for': 1.0986122886681098, 'true': 1.0986122886681098, 'shepherd': 1.0986122886681098, 'vicar': 1.0986122886681098, 'they': 1.0986122886681098, 'wa': 1.0986122886681098, 'refused': 1.0986122886681098, 'who': 1.0986122886681098, 'those': 1.0986122886681098, 'believe': 1.0986122886681098, 'he': 1.0986122886681098, 'arianist': 1.0986122886681098, 'termed': 1.0986122886681098, 'were': 1.0986122886681098, 'in': 0.4054651081081644, 'god': 0.4054651081081644, 'to': 0.4054651081081644, 'son': 0.4054651081081644, 'that': 0.4054651081081644, 'of': 0.0, 'the': 0.0}\n",
      "------------------------------ \n",
      "\n",
      "# correct synset: son.n.01\n",
      "# predicted synset naive: son.n.01 V\n",
      "# predicted synset lesk: son.n.01 V\n",
      "# predicted synset corpus lesk: son.n.01 V\n"
     ]
    }
   ],
   "source": [
    "#verbose test print\n",
    "token,token_list =get_random_word(data,only_nouns=True)\n",
    "print(\"# word: \",token[\"word\"])\n",
    "print(\"# sentence: \",[t[\"word\"]for t in token_list])\n",
    "print(\"# extracted context: \",get_context(token_list))\n",
    "print(\"# signature of each synset (No corpus examples):\")\n",
    "synsets=wn.synsets(token['word'],pos=token[\"pos\"])\n",
    "for syn in synsets:\n",
    "    signature=get_signature(syn)\n",
    "    overlap=compute_overlap(signature,get_context(token_list))\n",
    "    print(\" -\",syn.name(),\" overlap: \",overlap, \" signature: \",signature)\n",
    "\n",
    "print(\"\\n# signature of each synset (with corpus examples):\")\n",
    "corpus_examples=get_corpus_examples(synsets,data)\n",
    "for syn in synsets:\n",
    "    signature=get_signature(syn,corpus_examples[syn.name()])\n",
    "    overlap=compute_overlap(signature,get_context(token_list),weighted=True)\n",
    "    print(\" -\",syn.name(),\" overlap:\",round(overlap,4), \" signature: \",signature)\n",
    "\n",
    "word = token['word']\n",
    "pos=token['pos']\n",
    "correct_synset = token['syn'].name()\n",
    "def check_prediction(prediction):\n",
    "    return \"V\" if prediction == correct_synset else \"X\"\n",
    "print(\"-\"*30,\"\\n\")\n",
    "print(\"# correct synset:\", correct_synset)\n",
    "print(\"# predicted synset naive:\", (predicted_synset := naive_wsd(word).name()), check_prediction(predicted_synset))\n",
    "print(\"# predicted synset lesk:\", (predicted_synset := lesk(word,pos,token_list).name()), check_prediction(predicted_synset))\n",
    "print(\"# predicted synset corpus lesk:\", (predicted_synset := lesk(word,pos, token_list,data).name()), check_prediction(predicted_synset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Testing\n",
    "test on 50 phrases from SemCor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking all the data from the semcor corpus\n",
    "data=[[c for c in s] for s in semcor.tagged_sents(tag='both')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive: 0.76 lesk: 0.58 corpus_lesk: 0.96\n"
     ]
    }
   ],
   "source": [
    "#test on N phrases naive,lesk and corpus_lesk\n",
    "def testN(data,only_nouns=False,N=50,gemini=False):\n",
    "    c_naive=0\n",
    "    c_lesk=0\n",
    "    c_corpus_lesk=0\n",
    "    c_gemini_lesk=0\n",
    "    for i in range(N):\n",
    "        token,token_list=get_random_word(data,only_nouns)\n",
    "        correct_syn=token['syn']\n",
    "        word=token['word']\n",
    "        pos=token['pos']\n",
    "        if  naive_wsd(word)==correct_syn:\n",
    "            c_naive+=1\n",
    "        if lesk(word,pos,token_list)==correct_syn:\n",
    "            c_lesk+=1\n",
    "        if lesk(word,pos,token_list,data)==correct_syn:\n",
    "            c_corpus_lesk+=1\n",
    "        if gemini:\n",
    "            if lesk(word,pos,token_list,data,gemini)==correct_syn:\n",
    "                c_gemini_lesk+=1\n",
    "        \n",
    "    return c_naive/N,c_lesk/N,c_corpus_lesk/N,c_gemini_lesk/N\n",
    "\n",
    "accuracy=testN(data,only_nouns=True)\n",
    "print(\"naive:\",accuracy[0],\"lesk:\",accuracy[1],\"corpus_lesk:\",accuracy[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0 : 0.42 0.52 0.7\n",
      "Test 1 : 0.34 0.54 0.78\n",
      "Test 2 : 0.3 0.42 0.72\n",
      "Test 3 : 0.42 0.5 0.78\n",
      "Test 4 : 0.36 0.46 0.74\n",
      "Test 5 : 0.48 0.36 0.8\n",
      "Test 6 : 0.34 0.56 0.76\n",
      "Test 7 : 0.38 0.36 0.74\n",
      "Test 8 : 0.52 0.5 0.74\n",
      "Test 9 : 0.38 0.34 0.8\n",
      "----Avarage accuracy: ----\n",
      "naive: 0.394 lesk: 0.456 corpus_lesk: 0.756\n"
     ]
    }
   ],
   "source": [
    "#execute k=10 times the WSD algorithms on N=50 phrases, and return the average accuracy\n",
    "def full_test(k=10):\n",
    "    naive_results=0\n",
    "    lesk_results=0\n",
    "    corpus_lesk_results=0\n",
    "    for i in range(k):\n",
    "        n,l,cl,_ =testN(data)\n",
    "        print(\"Test \"+str(i)+\" :\",n,l,cl)\n",
    "        naive_results+=n\n",
    "        lesk_results+=l\n",
    "        corpus_lesk_results+=cl\n",
    "\n",
    "    return(round(naive_results/k,3),round(lesk_results/k,3),round(corpus_lesk_results/k,3))\n",
    "\n",
    "accuracy=full_test() #takes >10 minutes\n",
    "print(\"----Avarage accuracy: ----\")\n",
    "print(\"naive:\",accuracy[0],\"lesk:\",accuracy[1],\"corpus_lesk:\",accuracy[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Gemini extension\n",
    "WSD using Lesk corpus + additional examples generated by Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# IMPORTANT: USE A VPN TO ACCESS GEMINI\n",
    "API_KEY = \"abc123\"  # insert your own API key\n",
    "genai.configure(api_key=API_KEY)\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "gen_config = genai.types.GenerationConfig(temperature=0.7)  # other parameters are fine by default\n",
    "\n",
    "# often the security blocks are too restrictive, very often it didn't generate examples, so I set them to none\n",
    "safe_config = [\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        \"threshold\": \"BLOCK_NONE\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": \"BLOCK_NONE\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\"\n",
    "    },\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt engineering One-shot following the COSTAR framework\n",
    "- (C) Context: \"You are a smart lexicographer who wants to improve the world's best dictionary.\"\n",
    "- (O) Objective: \"Given a definition and some examples for a list of words, write 5 more examples that best represent the meaning of each word.\"\n",
    "- (S) Style: \"...a friendly, clear style to make the examples sound natural and easy to understand.\"\n",
    "- (T) Tone: \"Use a colloquial tone...\"\n",
    "- (A) Audience: \"For language learners who seek clear and relatable usage of the words.\"\n",
    "- (R) Response: \"Write the examples in plain text and insert them into a dictionary in the following format: {\"synset1\": [\"example1\", \"example2\", \"example3\", \"example4\", \"example5\"], \"synset2\": [\"example1\",...],...} [...] Do not use newlines.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(synsets):\n",
    "    prompt=\"\"\"\n",
    "    You are a smart lexicographer who wants to improve the world's best dictionary. Given a definition and some examples for a list of word, write 5 more examples that best represent the meaning of each word. \n",
    "    Use a colloquial tone and a friendly, clear style to make the examples sound natural and easy to understand for language learners who seek clear and relatable usage of the words\n",
    "    Write the examples in plain text and insert them into a dictionary in the following format: {\"synset1\": [\"example1\", \"example2\", \"example3\", \"example4\", \"example5\"],\"synset2\": [\"example1\",...],...}\n",
    "    Your response will be used as input for a program, so you MUST respect this format. Do not use newlines.\n",
    "\n",
    "    **example of input**\n",
    "    synset= \"bank.n.01\"\n",
    "    Term: \"bank\"\n",
    "    synonyms: \"depository_financial_institution\", \"banking_concern\", \"banking_company\"\n",
    "    Definition: \"a financial institution that accepts deposits and channels the money into lending activities\"\n",
    "    Examples: [\"he cashed a check at the bank\", \"that bank holds the mortgage on my home\"]\n",
    "\n",
    "    **your new 5 examples**\n",
    "    Example of Output: {\"bank.n.01\":[\"she opened a savings account at the local bank\", \"the bank approved his loan for the new car\", \"they went to the bank to deposit their paychecks\", \"the bank's interest rates for loans are very competitive\", \"after losing her debit card, she reported it to the bank immediately\"]}\n",
    "    \n",
    "    **Input:**\n",
    "    \"\"\"\n",
    "    for syn in synsets:\n",
    "        prompt+=\"\"\"\n",
    "        Term: \"\"\" + syn.lemmas()[0].name()+ \"\"\"  \n",
    "        synonyms: \"\"\" + str([lemma.name() for lemma in syn.lemmas()[1:]]) + \"\"\"\n",
    "        Definition: \"\"\" + str(syn.definition()) + \"\"\"\n",
    "        Examples: \"\"\" + str(syn.examples()) + \"\"\"\n",
    "        \"\"\"\n",
    "    prompt+=\"\"\"\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# given a list of synsets, returns a dictionary where each synset is associated with a list of usage examples generated with Gemini\n",
    "def get_gemini_examples(synsets):\n",
    "    prompt = get_prompt(synsets)\n",
    "    # Gemini API call\n",
    "    response = model.generate_content(prompt, generation_config=gen_config, safety_settings=safe_config)\n",
    "    time.sleep(1)  # to avoid exceeding the request limit\n",
    "    # if the generation didn't go well (e.g., safety reasons, retry up to 2 times)\n",
    "    if response.candidates[0].finish_reason > 1:\n",
    "        for i in range(2):\n",
    "            print(\"Error, finish reason: \", response.candidates[0].finish_reason)\n",
    "            response = model.generate_content(prompt, generation_config=gen_config)\n",
    "            if response.candidates[0].finish_reason == 1:\n",
    "                break\n",
    "        return None\n",
    "    # check that the obtained string can be converted into a list\n",
    "    try:\n",
    "        example_list = eval(response.text)\n",
    "        return example_list\n",
    "    except:\n",
    "        print(\"Error, output not formatted correctly\")\n",
    "        print(response.text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive: 0.4 lesk: 0.6 corpus_lesk: 0.8 gemini_corpus_lesk: 0.8\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "accuracy=testN(data,only_nouns=False,N=5,gemini=True)\n",
    "print(\"naive:\",accuracy[0],\"lesk:\",accuracy[1],\"corpus_lesk:\",accuracy[2],\"gemini_corpus_lesk:\",accuracy[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TLNenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
