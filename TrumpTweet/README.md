## Tweetting like a Trump
This is a simple university project for my NLP course.\
The goal is to use simple language models (N-grams) to model the language of a character and generate new sentences in their style.\
In this case study, the character is Donald Trump, and the dataset is a collection of his tweets.\

### Learning
N-grams are obtained from the sentences; an n-gram consists of n consecutive words, structured as $((w_{1}, ..., w_{n-1}), w_{n})$.\
This way, we already have a division between **context** and **word to predict**.\
Special tokens like padding are added to indicate the start `<S>` and the end `</S>` of a sentence.\
Transition probabilities are calculated for each word given its context, counting the occurrences of each n-gram and each context.

### N-gram probability: $p(w_i|w_{i-1}...w_{i-(n-1)})= \frac{C(w_{i-(n-1)}...w_{i-1},w_i)}{C(w_{i-(n-1)...w_{i-1}})}$
### 2-gram probability: $p(w_i|w_{i-1})= \frac{C(w_{i-1},w_i)}{C(w_{i-1})}$

### Decoding
A sentence is generated by selecting one word at a time given the previous context, until `</S>` is generated or a maximum length is reached.\
The selection can be **deterministic**: the word with the **highest probability** is chosen.\
Or **semi-random**: a word is chosen based on the **probability distribution**, allowing the generation of different sentences.

#### Temperature
The "randomness" of the selection can be adjusted using **temperature**, which alters the probability distribution with a **softmax** function.\
Softmax allows you to **intensify** or **smooth** the probability of words, based on the temperature [1,1000].\
For temp=0, the choice is forced to be deterministic; for low temp, the choice is almost deterministic; for high temp, the choice is random.\
Empirically: for temp=1000, we obtain a **uniform distribution**; however, even at temp=100, many nonsensical sentences are generated.

#### Initial Context
If no initial context is provided, the sentence starts with n-1 `<S>` tokens.\
If an initial context is provided, the sentence starts from its last n-1 words; if it's too short, `<S>` tokens are added until it reaches n-1 length.\
The presence of a user-provided context introduces an additional problem: the context might **not be present in the dataset**.\
In this case, to predict the next word, the system tries to use a **shorter context** by employing an (n-1)-gram.\
For example, (*some,people,do*) might not be present, but (*people,do*) might be.\
If the smaller context (a single word) is not present, it means the word is not present in the entire dataset, meaning it is an **unknown word**.\
In this case, the probability of the next word given an unknown word is estimated as $P(word|\text{UNK}) = \frac{C(\text{UNK},word)}{C(\text{UNK})}$\
For this estimate, words that **occur only once** in the dataset were used, considering them "unknown."

### Results
Donald Trump's **repetitive language** is captured well enough by a **simple tool** like n-grams and with such a limited dataset.\
The use of **temperature** to control the variance in word choice allows the generation of not only coherent but also plausible sentences for the character.\
The possibility of inserting a **custom initial context** further increases the variety of sentences that can be generated.\
Some significant examples have been collected in *TrumpTweets_results.txt*\
Empirically, **the best results** were obtained **with temp [1,50]**.\
Unfortunately, with increasingly larger n, the results do not improve, and the system starts to repeat entire sentences from the dataset.\
The code could be computationally optimized to avoid calculating probabilities that have already been computed or to avoid searching for increasingly smaller contexts when it is already known that they contain unknown words.